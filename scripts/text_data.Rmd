---
title: "text_data"
author: "Kate Mills"
date: "03/02/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

NOTE: If there is already a text_data.csv in the processed text data folder, then you can skip ahead to {r remove duplicates} or {r sentiments}

Load required packages and scripts (no input needed)
```{r setup, include=FALSE}
packages <- c( "tidyverse","ggmap",
               "tidytext","geosphere",
               "lubridate","Imap","modeest")
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, library, character.only = TRUE)

# Kate's custom theme
theme_kate <- function () { 
    theme_bw() +
  theme_minimal(base_size = 14, base_family = "Avenir") +
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        legend.position="bottom")
}
# So publicly avail code does not show location, first make /org/workdir.txt in your current directory, and first line should be location of data. End with a carriage return otherwise read.table won't work:
workdir<-as.character(read.table((paste0(getwd(),"/org/workdir.txt")))[[1]])
```

Extracting text data from decrypted files
```{r extract}
# Participant ID to device ID
pidsid<-read.csv(paste0(workdir,"raw_data/demographics_ids/pid_sid.csv"),header = T,stringsAsFactors = F) %>%
  #rename(pid="Ã¯..pid") %>% 
  mutate(pid=gsub("[^0-9]", "",pid))

# Generate list of subs
subs<-bind_rows((as.data.frame(list.files(path = paste0(workdir,"raw_data/keyboard/keyboard_logs_fall2016/"))) %>% 
                   mutate(sid=.[[1]]))[-1],
                (as.data.frame(list.files(path = paste0(workdir,"raw_data/keyboard/keyboard_logs_winter2017/"))) %>% 
                   mutate(sid=.[[1]]))[-1]) %>%
  distinct(sid) %>%
  filter(!grepl(".py",sid))

# some subs having trouble, so excluding for now
subs<-subs[-c(10), ] 

# Extract text data
extract_text=function(sid){
  if(file.exists(path=paste0(workdir,"raw_data/keyboard/keyboard_logs_fall2016/",sid,"/"))){
  files2016<-(as.data.frame(list.files(path=paste0(workdir,"raw_data/keyboard/keyboard_logs_fall2016/",sid,"/"),pattern = "log.decrypted")) %>%
                mutate(file=paste0(paste0(workdir,"raw_data/keyboard/keyboard_logs_fall2016/",sid,"/",.[[1]]))))}
  if(file.exists(path=paste0(workdir,"raw_data/keyboard/keyboard_logs_winter2017/",sid,"/"))){
    files2017<-(as.data.frame(list.files(path=paste0(workdir,"raw_data/keyboard/keyboard_logs_winter2017/",sid,"/"),pattern = "log.decrypted")) %>%
                mutate(file=paste0(workdir,"raw_data/keyboard/keyboard_logs_winter2017/",sid,"/",.[[1]])))}
  if(exists("files2016") & exists("files2017")) {
    files<-append(files2016$file,files2017$file)
  } else if(exists("files2016") & !exists("files2017")){
      files<-files2016$file
    } else if(exists("files2017")& !exists("files2016")){
      files<-files2017$file
    }
  rm(files2016,files2017)
  
  output<-lapply(X = files,FUN = function(file){
    if(nchar(read.csv(paste0(file),header = FALSE,sep = " ",stringsAsFactors = FALSE)$V5[[1]])<11){
      transform1<-read.csv(file,
                           header = FALSE,
                           sep = " ",
                           stringsAsFactors = FALSE,
                           fill = TRUE)
      numcols<-length(transform1)
      transform2<-reshape(transform1, direction="long", sep='', varying=paste0('V', 6:length(transform1))) %>%
        filter(!V=="") %>%
        mutate(startdate=substring(V1,3,last=nchar(V1)),
               starttime=substring(V2,1,last=(nchar(V2)-1)),
               enddate=substring(V4,2,last=nchar(V4)),
               endtime=substring(V5,1,last=8),
               word=V) %>%
        select(-contains("V"),-id,-time) %>%
        arrange(startdate,starttime)
      transform2
    } else if(nchar(read.csv(paste0(file),header = FALSE,sep = " ",stringsAsFactors = FALSE)$V5[[1]])>11){
      transform1<-read.csv(file,
                           header = FALSE,
                           sep = " ",
                           stringsAsFactors = FALSE,
                           fill = TRUE)
      numcols<-length(transform1)
      transform2<-reshape(transform1, direction="long", sep='', varying=paste0('V', 8:length(transform1))) %>%
        filter(!V=="") %>%
        mutate(startdate=substring(V1,3,last=nchar(V1)),
               starttime=substring(V2,1,last=(nchar(V2)-1)),
               enddate=substring(V4,2,last=nchar(V4)),
               endtime=substring(V5,1,last=8),
               word=V) %>%
        select(-contains("V"),-id,-time) %>%
        arrange(startdate,starttime)
      transform2
      }
    })
  
  output.df<-data.table::rbindlist(output, fill=TRUE)
  output.df
  if (nrow(output.df)>0){cbind(sid,output.df)
  } else {
    output<-cbind(sid,NA,NA,NA,NA,NA)
    colnames(output)<-c("sid","startdate","starttime","enddate","endtime","word")
    output.df<-as.data.frame(output)
    output.df
  }
}
extract_text_out<-lapply(subs,extract_text)
extract_text_out.df<-data.table::rbindlist(extract_text_out) %>%
  na.omit(.)
extract_text_out.df<-left_join(extract_text_out.df,pidsid) %>%
  select(-sid) %>%
  select(pid,startdate,starttime,enddate,endtime,word)


write.csv(extract_text_out.df,file = paste0(workdir,"output/processed_text_data/text_data.csv"),row.names = FALSE)
print(paste0("There are ",length(unique(extract_text_out.df$pid))," EASE participants with text data"))
```



```{r remove duplicates}
extract_text_out.df<-read.csv(paste0(workdir,"output/processed_text_data/text_data.csv"))

cleaned_dupes<-extract_text_out.df %>%
  distinct(sid, startdate, starttime, word, .keep_all = TRUE) %>%
  filter(!startdate=="") %>%
  filter(!grepl(pattern = " - ",x = word)) %>%
  filter(grepl("-",startdate))

print(paste0("There are ",length(unique(cleaned_dupes$pid))," EASE participants with text data"))
write.csv(cleaned_dupes,file = paste0(workdir,"output/processed_text_data/text_data_cleanwyze.csv"),row.names = FALSE)
```

Analyze data - for more info, see https://www.tidytextmining.com/sentiment.html
```{r sentiments}
cleaned_dupes<-read.csv(paste0(workdir,"output/processed_text_data/text_data_cleaned.csv"))

# Note: MB wanted to get total (any) words by participant and the only way she could figure out how is to hard code by participant ID (sorry)"

P001_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"001"' & cleaned_dupes$week==1), ])
P001_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"001"' & cleaned_dupes$week==2), ])
P003_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"003"' & cleaned_dupes$week==1), ])
P003_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"003"' & cleaned_dupes$week==2), ])
P006_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"006"' & cleaned_dupes$week==1), ])
P006_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"006"' & cleaned_dupes$week==2), ])
P009_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"009"' & cleaned_dupes$week==1), ])
P009_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"009"' & cleaned_dupes$week==2), ])
P011_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"011"' & cleaned_dupes$week==1), ])
P011_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"011"' & cleaned_dupes$week==2), ])
P012_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"012"' & cleaned_dupes$week==1), ])
P012_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"012"' & cleaned_dupes$week==2), ])
P013_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"013"' & cleaned_dupes$week==1), ])
P013_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"013"' & cleaned_dupes$week==2), ])
P014_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"014"' & cleaned_dupes$week==1), ])
P014_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"014"' & cleaned_dupes$week==2), ])
P015_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"015"' & cleaned_dupes$week==1), ])
P015_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"015"' & cleaned_dupes$week==2), ])
P016_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"016"' & cleaned_dupes$week==1), ])
P016_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"016"' & cleaned_dupes$week==2), ])
P017_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"017"' & cleaned_dupes$week==1), ])
P017_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"017"' & cleaned_dupes$week==2), ])
P018_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"018"' & cleaned_dupes$week==1), ])
P018_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"018"' & cleaned_dupes$week==2), ])
P019_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"019"' & cleaned_dupes$week==1), ])
P019_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"019"' & cleaned_dupes$week==2), ])
P020_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"020"' & cleaned_dupes$week==1), ])
P020_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"020"' & cleaned_dupes$week==2), ])
P023_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"023"' & cleaned_dupes$week==1), ])
P023_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"023"' & cleaned_dupes$week==2), ])
P024_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"024"' & cleaned_dupes$week==1), ])
P024_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"024"' & cleaned_dupes$week==2), ])
P025_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"025"' & cleaned_dupes$week==1), ])
P025_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"025"' & cleaned_dupes$week==2), ])
P027_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"027"' & cleaned_dupes$week==1), ])
P027_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"027"' & cleaned_dupes$week==2), ])
P029_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"029"' & cleaned_dupes$week==1), ])
P029_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"029"' & cleaned_dupes$week==2), ])
P031_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"031"' & cleaned_dupes$week==1), ])
P031_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"031"' & cleaned_dupes$week==2), ])
P033_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"033"' & cleaned_dupes$week==1), ])
P033_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"033"' & cleaned_dupes$week==2), ])
P034_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"034"' & cleaned_dupes$week==1), ])
P034_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"034"' & cleaned_dupes$week==2), ])
P035_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"035"	' & cleaned_dupes$week==1), ])
P035_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"035"	' & cleaned_dupes$week==2), ])
P036_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"036"' & cleaned_dupes$week==1), ])
P036_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"036"' & cleaned_dupes$week==2), ])
P037_tot_w1 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"037"' & cleaned_dupes$week==1), ])
P037_tot_w2 = nrow(cleaned_dupes[which(cleaned_dupes$pid=='"037"' & cleaned_dupes$week==2), ])

immunology_plus_dataset$tot_words_t1 <- rbind(P001_tot_w1,P003_tot_w1, P006_tot_w1, P009_tot_w1, P011_tot_w1, P012_tot_w1, P013_tot_w1, P014_tot_w1, P015_tot_w1, P016_tot_w1, P017_tot_w1, P018_tot_w1, P019_tot_w1, P020_tot_w1, P023_tot_w1, P024_tot_w1, P025_tot_w1, P027_tot_w1, P029_tot_w1, P031_tot_w1, P033_tot_w1, P034_tot_w1, P035_tot_w1, P036_tot_w1, P037_tot_w1)

immunology_plus_dataset$tot_words_t2 <- rbind(P001_tot_w2,P003_tot_w2, P006_tot_w2, P009_tot_w2, P011_tot_w2, P012_tot_w2, P013_tot_w2, P014_tot_w2, P015_tot_w2, P016_tot_w2, P017_tot_w2, P018_tot_w2, P019_tot_w2, P020_tot_w2, P023_tot_w2, P024_tot_w2, P025_tot_w2, P027_tot_w2, P029_tot_w2, P031_tot_w2, P033_tot_w2, P034_tot_w2, P035_tot_w2, P036_tot_w2, P037_tot_w2)

# Now get affective words (Kate did):
get_sentiments("bing")

get_sentiments("afinn")

out1<-cleaned_dupes %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  top_n(20) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col(fill = "purple") +
  coord_flip()

cleaned_dupes %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(sentiment, word, sort = TRUE)

cleaned_dupes %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(pid) %>% 
  count(value, word, sort = TRUE)

head(cleaned_dupes)

cleaned_dupes$startdate <- ymd(cleaned_dupes$startdate)

cleaned_dupes$day <- weekdays(cleaned_dupes$startdate)

cleaned_dupes <- cleaned_dupes %>%
  mutate(fullstart=ymd_hms(paste0(startdate," ",starttime)),
         fullstop=ymd_hms(paste0(enddate," ",endtime)))

afinn_week<-cleaned_dupes %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(day) %>% 
  summarize(mean_score = mean(value))

bing_week<-cleaned_dupes %>% 
  inner_join(get_sentiments("bing")) %>%
  group_by(day) %>% 
  count(.,sentiment)

# Plots
bing_week_graph<-ggplot(data=bing_week, 
                        aes(x=factor(day,levels=c("Monday",
                                                  "Tuesday",
                                                  "Wednesday",
                                                  "Thursday",
                                                  "Friday",
                                                  "Saturday",
                                                  "Sunday")),
                            y=n,
                            group=sentiment,
                            colour=sentiment)) +
  scale_color_manual(values=c("#8C2155", "#75BBA7"))+
  ylab("frequency") +
  xlab("")+
  geom_line(size=1) +
  theme_kate() +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))
bing_week_graph

ggsave(filename=paste0(workdir,"output/scored_text_data/graphs/bing_week_graph.png"),plot=bing_week_graph, width=6, height=5, units='in', dpi=300)


# Join words and geolocation
pidsid<-read.csv(paste0(workdir,"raw_data/demographics_ids/pid_sid.csv"),header = T,stringsAsFactors = F) %>%
  mutate(pid=gsub("[^0-9]", "",pid)) %>%
  mutate(pid=sub("(^|[^0-9])0+", "\\1", pid, perl = TRUE))

geolocation<-left_join((geodata_distance_home %>% mutate(sid=sub)),pidsid) %>%
  mutate(pid=as.integer(pid))%>%
  select(-sub,-sid)

word_geo<-left_join(geolocation,cleaned_dupes,
                   by=c("pid", "fullstart", "fullstop"))

bing_timeofday<-word_geo %>% 
  inner_join(get_sentiments("bing")) %>%
  group_by(timeofday) %>% 
  count(.,sentiment)

# Plots
bing_timeofday_graph<-ggplot(data=bing_timeofday, 
                        aes(x=factor(timeofday,levels=c("early morning",
                                                  "day",
                                                  "evening",
                                                  "night")),
                            y=n,
                            group=sentiment,
                            colour=sentiment)) +
  scale_color_manual(values=c("#8C2155", "#75BBA7"))+
  ylab("frequency") +
  xlab("")+
  geom_line(size=1) +
  theme_kate() +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))
bing_timeofday_graph

bing_home<-word_geo %>% 
  inner_join(get_sentiments("bing")) %>%
  group_by(home) %>% 
  count(.,sentiment)


```

Mean text by subject (Michelle)
```{r subj_text}
afinn_subj<-cleaned_dupes %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(pid) %>% 
  summarize(mean_score = mean(value))

afinn_subj_week<-cleaned_dupes %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(pid, week) %>% 
  summarize(mean_score = mean(value))

bing_subj<-cleaned_dupes %>% 
  inner_join(get_sentiments("bing")) %>%
  group_by(pid) %>% 
  count(.,sentiment)

bing_subj_week<-cleaned_dupes %>% 
  inner_join(get_sentiments("bing")) %>%
  group_by(pid, week) %>% 
  count(.,sentiment)

write.csv(afinn_subj, file = "A:/Adapt/Studies/EASE/output/scored_text_data/afinn_scores.csv")
write.csv(afinn_subj_week, file = "A:/Adapt/Studies/EASE/output/scored_text_data/afinn_scores_byweek.csv")
write.csv(bing_subj, file = "A:/Adapt/Studies/EASE/output/scored_text_data/bing_scores.csv")
write.csv(bing_subj_week, file = "A:/Adapt/Studies/EASE/output/scored_text_data/bing_scores_byweek.csv")
```

